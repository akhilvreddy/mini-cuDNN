{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470003b7",
   "metadata": {},
   "source": [
    "# mini-cuDNN: Conv2d Validation & Benchmark Notebook\n",
    "\n",
    "**What I am doing here**\n",
    "1. Verify CUDA/PyTorch environment\n",
    "2. Install my local repo as an editable package (`pip install -e .`)\n",
    "3. Validate correctness vs `torch.nn.functional.conv2d`\n",
    "4. Benchmark my kernel and compare to PyTorch native\n",
    "\n",
    "Since I am writing this on local first, there are some things that I need to keep in mind for google colab.\n",
    "\n",
    "- Upload to Google Drive\n",
    "- Mount Drive into Colab\n",
    "- cd into project folder\n",
    "- Setup via bash commands\n",
    "- Run .ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, platform, sys, subprocess, os, time\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}  |  Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    print(f\"Torch CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66f041",
   "metadata": {},
   "source": [
    "To install the extension we need to first make sure we are at the root of the repo (`mydnn/` must be accessible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb09e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change into your repo folder if needed.\n",
    "# If you cloned above, use TARGET_DIR; otherwise, set REPO_PATH manually.\n",
    "REPO_PATH = TARGET_DIR if os.path.exists(TARGET_DIR) else \".\"\n",
    "\n",
    "print(\"Using repo path:\", os.path.abspath(REPO_PATH))\n",
    "os.chdir(REPO_PATH)\n",
    "\n",
    "# Install editable\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09f4d4",
   "metadata": {},
   "source": [
    "### Comparing vs. `torch.nn.functional.conv2d`\n",
    "\n",
    "Here, we test this against a couple of different shapes just as a first run check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "import mydnn\n",
    "\n",
    "def check_case(N=2, C=3, H=16, W=17, K=4, R=3, S=3, sh=1, sw=1, ph=1, pw=1, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    x = torch.randn(N, C, H, W, device=\"cuda\", dtype=torch.float32)\n",
    "    w = torch.randn(K, C, R, S, device=\"cuda\", dtype=torch.float32)\n",
    "    y_ref = F.conv2d(x, w, stride=(sh, sw), padding=(ph, pw))\n",
    "    y = mydnn.conv2d_naive(x, w, sh, sw, ph, pw)\n",
    "    max_abs = (y - y_ref).abs().max().item()\n",
    "    max_rel = ((y - y_ref).abs() / (y_ref.abs() + 1e-6)).max().item()\n",
    "    print(f\"Shape NCHW=({N},{C},{H},{W}) KRS=({K},{R},{S}) stride=({sh},{sw}) pad=({ph},{pw})  -> max_abs={max_abs:.2e}, max_rel={max_rel:.2e}\")\n",
    "    return max_abs, max_rel\n",
    "\n",
    "# Run a few checks\n",
    "cases = [\n",
    "    (1,1,8,8, 1,3,3, 1,1, 0,0),\n",
    "    (2,3,16,17, 4,3,3, 1,1, 1,1),\n",
    "    (4,8,15,13, 8,5,5, 2,2, 1,1),\n",
    "]\n",
    "for (N,C,H,W,K,R,S, sh,sw, ph,pw) in cases:\n",
    "    check_case(N,C,H,W,K,R,S, sh,sw, ph,pw, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f8510",
   "metadata": {},
   "source": [
    "## Benchmarking my layer vs. PyTorch native\n",
    "\n",
    "Here I am benchmarking the time it takes to crunch a conv2d using my layer vs native PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce24cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "\n",
    "def time_it(fn, iters=20, warmup=5):\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = fn()\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(iters):\n",
    "        _ = fn()\n",
    "    torch.cuda.synchronize()\n",
    "    dt_ms = (time.time() - t0) * 1000.0 / iters\n",
    "    return dt_ms\n",
    "\n",
    "def flops_conv2d(N,C,H,W,K,R,S, Ho,Wo):\n",
    "    # Approx FLOPs for conv: N * Ho * Wo * K * (C*R*S*2)  (mul+add ~ 2 ops)\n",
    "    return N * Ho * Wo * K * (C * R * S * 2.0)\n",
    "\n",
    "def bench_case(N,C,H,W,K,R,S, sh,sw, ph,pw, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    x = torch.randn(N, C, H, W, device=\"cuda\", dtype=torch.float32)\n",
    "    w = torch.randn(K, C, R, S, device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "    Ho = (H + 2*ph - R)//sh + 1\n",
    "    Wo = (W + 2*pw - S)//sw + 1\n",
    "\n",
    "    # Capture callables\n",
    "    def call_mydnn():\n",
    "        return mydnn.conv2d_naive(x, w, sh, sw, ph, pw)\n",
    "    def call_torch():\n",
    "        return torch.nn.functional.conv2d(x, w, stride=(sh, sw), padding=(ph, pw))\n",
    "\n",
    "    t_mydnn = time_it(call_mydnn)\n",
    "    t_torch = time_it(call_torch)\n",
    "\n",
    "    gflops = flops_conv2d(N,C,H,W,K,R,S, Ho,Wo) / 1e9\n",
    "    mydnn_gflops = gflops / (t_mydnn / 1000.0)\n",
    "    torch_gflops = gflops / (t_torch / 1000.0)\n",
    "\n",
    "    print(f\"[NCHW=({N},{C},{H},{W}) KRS=({K},{R},{S}) stride=({sh},{sw}) pad=({ph},{pw})]\")\n",
    "    print(f\"  mydnn: {t_mydnn:.2f} ms   ~ {mydnn_gflops:.2f} GFLOP/s\")\n",
    "    print(f\"  torch: {t_torch:.2f} ms   ~ {torch_gflops:.2f} GFLOP/s\")\n",
    "\n",
    "    return {\n",
    "        \"N\":N, \"C\":C, \"H\":H, \"W\":W, \"K\":K, \"R\":R, \"S\":S,\n",
    "        \"sh\":sh, \"sw\":sw, \"ph\":ph, \"pw\":pw,\n",
    "        \"Ho\":Ho, \"Wo\":Wo,\n",
    "        \"t_mydnn_ms\": t_mydnn,\n",
    "        \"t_torch_ms\": t_torch,\n",
    "        \"mydnn_gflops\": mydnn_gflops,\n",
    "        \"torch_gflops\": torch_gflops,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ab27d",
   "metadata": {},
   "source": [
    "## Concluding notes\n",
    "\n",
    "- The naive kernel that I wrote is ***much*** slower than PyTorch and that makes sense. I wrote a thin conv2d parallelization layer but true PyTorch uses optimized cuDNN/cuBLAS under the hood with an autotuner (smart router)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
